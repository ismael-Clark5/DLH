{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link to github repo with the code: https://github.com/ismael-Clark5/DLH.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The subsequent paper endeavors to replicate and validate the findings posited by Jae Yong Ryu, Hyun Uk Kim, and Sang Yup Lee in their work titled \"Deep learning improves prediction of drug–drug and drug–food interactions\" (1).\n",
        "\n",
        "In the aforementioned study, the authors seek to address the multilabel classification issue of Drug-Drug Interaction (DDI) and Drug-Food Interaction (DFI) through the utilization of a deep learning framework named DeepDDI, utilizing the DDI gold standard dataset. This is a particularly pertinent area within the healthcare domain, given that DDIs are associated with approximately 30% of all Adverse Drug Effects (2), especially in light of the fact that 67% of elderly Americans are reportedly taking five or more medications, including prescription drugs, over-the-counter drugs, and dietary supplements (3).\n",
        "\n",
        "There have been two primary hurdles when addressing the challenges of DDIs and DFIs. For instance, preceding models prior to DeepDDI solely predict the probability of interaction between drugs without offering further insight into the pharmacological effects of the DDI. Additionally, many of these methodologies often necessitate a substantial amount of detailed drug information such as drug targets, interacting drugs, and side effects, which are frequently unavailable, as inputs to predict DDIs (4-10). Furthermore, prior methods developed have not been scrutinized with regard to analyzing DFIs.\n",
        "\n",
        "The original paper employed Random Forest Classifiers and K-Nearest Neighbors as alternatives to DeepDDI, and their performance was assessed and juxtaposed (1).\n",
        "\n",
        "The initial text introduced DeepDDI as a Deep Neural Network multi-label classifier to accurately discern DDIs and DFIs based on the DDI dataset. This work is innovative on multiple fronts. Initially, it capitalizes on DDI and its broad spectrum of compound types, many of which are natural products present in food, to predict Drug-Food Interactions. Additionally, it only necessitates the names of the two drugs under examination as input pairs, along with the simplified molecular-input line-entry system (SMILES) that delineates the structure of a chemical compound, to generate predictions.\n",
        "\n",
        "DeepDDI attains a 92.4% mean accuracy (1), marking a notable advancement over other machine learning methodologies utilized for benchmarking. DeepDDI introduces a pioneering approach to addressing DDI and DFI challenges, exhibiting exceptional outcomes that could potentially enhance healthcare services and yield financial savings in Drug Development by accurately discerning the effects they may induce when interacting with food or other medications.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "## Hypothesis\n",
        "Hypothesis 1: With a threshold of 0.47, DeepDDI acheives a 92.4% mean accuracy\n",
        "\n",
        "Hypothesis 2: With a total of 9 hidden layers and 2048 nodes per layer, DeepDDI achieved the best accuracy 92.4%\n",
        "\n",
        "Hypothesis 3: 100 epochs is the best number of training iterations to achieve these results\n",
        "\n",
        "## Experiements to run:\n",
        "\n",
        "  We will try to run the DeepDDI models with different values for number of epochs, number of hidden layers and nodes and different threshold values and we will compare our results with those of the paper. Currently, we are only trying to verify the results, given the length of time needed for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/pip/\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install torch\n",
        "!pip -q install matplotlib\n",
        "!pip -q install pandas\n",
        "!pip -q install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install scikit-learn\n",
        "!pip -q install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install wheel setuptools --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\placeholder.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\scatter.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_coo.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_csr.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\utils.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\logsumexp.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\softmax.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\std.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      running egg_info\n",
            "      writing torch_scatter.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_scatter.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_scatter.egg-info\\requires.txt\n",
            "      writing top-level names to torch_scatter.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_scatter._scatter_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-scatter\n",
            "ERROR: Could not build wheels for torch-scatter, which is required to install pyproject.toml-based projects\n",
            "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /packages/42/e2/cddf10a8e32a0b214918943e6173672c8ec11000e69c36dad8e6b141cb60/torch_sparse-0.6.18.tar.gz\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [56 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\add.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\bandwidth.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\cat.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\coalesce.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\convert.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\diag.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\eye.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\index_select.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\masked_select.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\matmul.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\metis.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\mul.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\narrow.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\permute.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\reduce.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\rw.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\saint.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\sample.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\select.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\spadd.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\spmm.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\spspmm.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\storage.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\tensor.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\transpose.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\typing.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\utils.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      copying torch_sparse\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_sparse\n",
            "      running egg_info\n",
            "      writing torch_sparse.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_sparse.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_sparse.egg-info\\requires.txt\n",
            "      writing top-level names to torch_sparse.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\css'\n",
            "      warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\html'\n",
            "      warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\tests'\n",
            "      warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\examples'\n",
            "      warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\benchmark'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      warning: no previously-included files matching '*' found under directory 'benchmark'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_sparse._convert_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-sparse\n",
            "ERROR: Could not build wheels for torch-sparse, which is required to install pyproject.toml-based projects\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /whl/torch-%7Btorch.__version__%7D.html\n",
            "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /whl/torch-%7Btorch.__version__%7D.html\n",
            "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /whl/torch-%7Btorch.__version__%7D.html\n",
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/torch-cluster/\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [32 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\fps.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\graclus.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\grid.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\knn.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\nearest.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\radius.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\rw.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\sampler.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\typing.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      running egg_info\n",
            "      writing torch_cluster.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_cluster.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_cluster.egg-info\\requires.txt\n",
            "      writing top-level names to torch_cluster.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_cluster._fps_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-cluster\n",
            "ERROR: Could not build wheels for torch-cluster, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "!pip -q uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip -q install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip -q install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip -q install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip -q install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/torch-scatter/\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\placeholder.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\scatter.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_coo.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_csr.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\utils.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\logsumexp.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\softmax.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\std.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      running egg_info\n",
            "      writing torch_scatter.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_scatter.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_scatter.egg-info\\requires.txt\n",
            "      writing top-level names to torch_scatter.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_scatter._scatter_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-scatter\n",
            "ERROR: Could not build wheels for torch-scatter, which is required to install pyproject.toml-based projects\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [32 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\fps.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\graclus.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\grid.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\knn.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\nearest.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\radius.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\rw.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\sampler.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\typing.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      running egg_info\n",
            "      writing torch_cluster.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_cluster.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_cluster.egg-info\\requires.txt\n",
            "      writing top-level names to torch_cluster.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_cluster._fps_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-cluster\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\placeholder.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\scatter.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_coo.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_csr.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\utils.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\logsumexp.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\softmax.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\std.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      running egg_info\n",
            "      writing torch_scatter.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_scatter.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_scatter.egg-info\\requires.txt\n",
            "      writing top-level names to torch_scatter.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_scatter._scatter_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-scatter\n",
            "ERROR: Could not build wheels for torch-cluster, torch-scatter, which is required to install pyproject.toml-based projects\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/torch-scatter/\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\placeholder.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\scatter.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_coo.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\segment_csr.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\utils.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      copying torch_scatter\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\logsumexp.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\softmax.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\std.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      copying torch_scatter\\composite\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_scatter\\composite\n",
            "      running egg_info\n",
            "      writing torch_scatter.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_scatter.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_scatter.egg-info\\requires.txt\n",
            "      writing top-level names to torch_scatter.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_scatter._scatter_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-scatter\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [32 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\fps.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\graclus.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\grid.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\knn.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\nearest.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\radius.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\rw.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\sampler.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\testing.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\typing.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      copying torch_cluster\\__init__.py -> build\\lib.win-amd64-cpython-310\\torch_cluster\n",
            "      running egg_info\n",
            "      writing torch_cluster.egg-info\\PKG-INFO\n",
            "      writing dependency_links to torch_cluster.egg-info\\dependency_links.txt\n",
            "      writing requirements to torch_cluster.egg-info\\requires.txt\n",
            "      writing top-level names to torch_cluster.egg-info\\top_level.txt\n",
            "      reading manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*' found under directory 'test'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
            "      running build_ext\n",
            "      C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
            "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
            "      building 'torch_cluster._fps_cpu' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for torch-cluster\n",
            "ERROR: Could not build wheels for torch-scatter, torch-cluster, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "%pip -q install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cpu.html\n",
        "%pip -q install torchdrug\n",
        "# %pip install chemicalx\n",
        "%pip -q install -e git+https://github.com/lucag2/chemicalx.git#egg=chemicalx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ismac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# import  packages you need\n",
        "import torch\n",
        "import numpy as np\n",
        "import chemicalx\n",
        "# from google.colab import drive\n",
        "from sklearn.metrics import *\n",
        "from chemicalx.data import DrugbankDDI, BatchGenerator\n",
        "from torch import nn\n",
        "from chemicalx.data import DrugPairBatch\n",
        "from chemicalx.models import Model\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: The following script is used in the chemicalx library in order to clean and retrieved the drugbankDDI dataset. The model that is defined below it is also from the chemicalx library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \"\"\"Download and pre-process the DrugBank drug-drug interaction dataset.\"\"\"\n",
        "\n",
        "# import math\n",
        "# from random import Random\n",
        "\n",
        "# import click\n",
        "# import pandas as pd\n",
        "# from utils import get_index, get_samples, get_tdc, write_artifacts\n",
        "\n",
        "\n",
        "# @click.command()\n",
        "# @click.option(\"--seed\", type=int, default=42, show_default=True, help=\"Random seed\")\n",
        "# @click.option(\"--ratio\", type=float, default=1.0, show_default=True, help=\"Negative sampling ratio\")\n",
        "# def main(seed: int, ratio: float):\n",
        "#     \"\"\"Download and pre-process the DrugBank DDI dataset.\"\"\"\n",
        "#     rng = Random(seed)\n",
        "#     input_directory, output_directory = get_tdc(\"drugbank\", \"drugbankddi\")\n",
        "\n",
        "#     positive_samples = pd.read_csv(\n",
        "#         input_directory.joinpath(\"drugbank.tab\"),\n",
        "#         sep=\"\\t\",\n",
        "#         usecols=[0, 1, 2, 4, 5],\n",
        "#         header=0,\n",
        "#         names=[\"drug_1\", \"drug_2\", \"context\", \"drug_1_smiles\", \"drug_2_smiles\"],\n",
        "#     )\n",
        "#     positive_samples[\"context\"] = positive_samples[\"context\"].map(lambda x: f\"context_{x:02}\")\n",
        "#     print(\"Number of positive samples:\", positive_samples.shape[0])\n",
        "#     print(\"Columns:\", positive_samples.columns)\n",
        "\n",
        "#     contexts = list(sorted(set(positive_samples[\"context\"].values.tolist())))\n",
        "#     print(\"Number of contexts:\", len(contexts))\n",
        "\n",
        "#     # Index drugs' SMILES and drug-drug-context triples\n",
        "#     drugs_raw, big_map = get_index(positive_samples)\n",
        "#     drugs_raw.update(\n",
        "#         {\n",
        "#             \"DB09323\": \"O.O.O.O.C(CNCC1=CC=CC=C1)NCC1=CC=CC=C1.[H][C@]12SC(C)(C)[C@@H](N1C(=O)[C@H]2NC(=O)CC1=CC=CC=C1)C(O)=O.[H][C@]12SC(C)(C)[C@@H](N1C(=O)[C@H]2NC(=O)CC1=CC=CC=C1)C(O)=O\",  # noqa:E501\n",
        "#             \"DB13450\": \"[O-]S(=O)(=O)C1=CC=CC=C1.[O-]S(=O)(=O)C1=CC=CC=C1.COC1=CC2=C(C=C1OC)[C@@H](CC1=CC(OC)=C(OC)C=C1)[N@@+](C)(CCC(=O)OCCCCCOC(=O)CC[N@@+]1(C)CCC3=C(C=C(OC)C(OC)=C3)[C@H]1CC1=CC(OC)=C(OC)C=C1)CC2\",  # noqa:E501\n",
        "#             \"DB09396\": \"O.OS(=O)(=O)C1=CC2=CC=CC=C2C=C1.CCC(=O)O[C@@](CC1=CC=CC=C1)([C@H](C)CN(C)C)C1=CC=CC=C1\",\n",
        "#             \"DB09162\": \"[Fe+3].OC(CC([O-])=O)(CC([O-])=O)C([O-])=O\",\n",
        "#             \"DB11106\": \"CC(C)(N)CO.CN1C2=C(NC(Br)=N2)C(=O)N(C)C1=O\",\n",
        "#             \"DB11630\": \"C1CC2=NC1=C(C3=CC=C(N3)C(=C4C=CC(=N4)C(=C5C=CC(=C2C6=CC(=CC=C6)O)N5)C7=CC(=CC=C7)O)C8=CC(=CC=C8)O)C9=CC(=CC=C9)O\",  # noqa:E501\n",
        "#             \"DB00958\": \"C1CC(C1)(C(=O)O)C(=O)O.[NH2-].[NH2-].[Pt+2]\",\n",
        "#             \"DB00526\": \"C1CCC(C(C1)[NH-])[NH-].C(=O)(C(=O)O)O.[Pt+2]\",\n",
        "#             \"DB13145\": \"C(C(=O)O)O.[NH2-].[NH2-].[Pt+2]\",\n",
        "#             \"DB00515\": \"N.N.Cl[Pt]Cl\",\n",
        "#         }\n",
        "#     )\n",
        "\n",
        "#     drugs = list(drugs_raw)\n",
        "#     print(\"Number of drugs:\", len(drugs))\n",
        "\n",
        "#     # Generate negative samples\n",
        "#     negative_samples = get_samples(\n",
        "#         rng=rng, n=int(math.ceil(ratio * positive_samples.shape[0])), drugs=drugs, contexts=contexts, big_map=big_map\n",
        "#     )\n",
        "\n",
        "#     labeled_triples = positive_samples[[\"drug_1\", \"drug_2\", \"context\"]]\n",
        "#     labeled_triples[\"label\"] = 1.0\n",
        "#     labeled_triples = pd.concat([labeled_triples, negative_samples])\n",
        "#     print(\"Number of total triples:\", labeled_triples.shape)\n",
        "#     labeled_triples.to_csv(output_directory.joinpath(\"labeled_triples.csv\"), index=False)\n",
        "\n",
        "#     write_artifacts(output_directory=output_directory, drugs_raw=drugs_raw, contexts=contexts)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Split: 0.5\n",
            "Train Dataset Size: 191808\n",
            "Total number of drugs in Train: 1706\n",
            "Total number of unique drug pairs in Train: 187091\n",
            "Label split in Train (Positive, Negative): (96011, 95797)\n",
            "Ratio of label split in Train (Positive, Negative): (0.5005578495161829, 0.4994421504838171)\n",
            "Test Dataset Size: 191808\n",
            "Total number of drugs in Test: 1706\n",
            "Total number of unique drug pairs in Test: 187068\n",
            "Label split in Test (Positive, Negative): (95797, 96011)\n",
            "Ratio of label split in Test (Positive, Negative): (0.4994421504838171, 0.5005578495161829)\n"
          ]
        }
      ],
      "source": [
        "loader = DrugbankDDI()\n",
        "\n",
        "context_set = loader.get_context_features()\n",
        "drug_set = loader.get_drug_features()\n",
        "triples = loader.get_labeled_triples()\n",
        "cross_validation_split = 0.5\n",
        "train, test = triples.train_test_split(train_size=cross_validation_split)\n",
        "\n",
        "generator = BatchGenerator(batch_size=1024,\n",
        "                           context_features=True,\n",
        "                           drug_features=True,\n",
        "                           drug_molecules=False,\n",
        "                           context_feature_set=context_set,\n",
        "                           drug_feature_set=drug_set,\n",
        "                           labeled_triples=train)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(train, test):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  print(f\"Cross Validation Split: {cross_validation_split}\")\n",
        "  print(f\"Train Dataset Size: {len(train)}\")\n",
        "  print(f\"Total number of drugs in Train: {train.get_drug_count()}\")\n",
        "  print(f'Total number of unique drug pairs in Train: {train.get_combination_count()}')\n",
        "  print(f\"Label split in Train (Positive, Negative): ({train.get_positive_count()}, {train.get_negative_count()})\")\n",
        "  print(f\"Ratio of label split in Train (Positive, Negative): ({train.get_positive_rate()}, {train.get_negative_rate()})\")\n",
        "  print(f\"Test Dataset Size: {len(test)}\")\n",
        "  print(f\"Total number of drugs in Test: {test.get_drug_count()}\")\n",
        "  print(f'Total number of unique drug pairs in Test: {test.get_combination_count()}')\n",
        "  print(f\"Label split in Test (Positive, Negative): ({test.get_positive_count()}, {test.get_negative_count()})\")\n",
        "  print(f\"Ratio of label split in Test (Positive, Negative): ({test.get_positive_rate()}, {test.get_negative_rate()})\")\n",
        "  return None\n",
        "\n",
        "calculate_stats(train, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "outputs": [],
      "source": [
        "__all__ = [\n",
        "    \"DeepDDI\",\n",
        "]\n",
        "\n",
        "\n",
        "class DeepDDI(Model):\n",
        "    \"\"\"An implementation of the DeepDDI model from [ryu2018]_.\n",
        "\n",
        "    .. seealso:: This model was suggested in https://github.com/AstraZeneca/chemicalx/issues/2\n",
        "\n",
        "    .. [ryu2018] Ryu, J. Y., *et al.* (2018). `Deep learning improves prediction\n",
        "       of drug–drug and drug–food interactions <https://doi.org/10.1073/pnas.1803294115>`_.\n",
        "       *Proceedings of the National Academy of Sciences*, 115(18), E4304–E4311.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        drug_channels: int,\n",
        "        hidden_channels: int = 2048,\n",
        "        hidden_layers_num: int = 9,\n",
        "        out_channels: int = 1,\n",
        "    ):\n",
        "        \"\"\"Instantiate the DeepDDI model.\n",
        "\n",
        "        :param drug_channels: The number of drug features.\n",
        "        :param hidden_channels: The number of hidden layer neurons.\n",
        "        :param hidden_layers_num: The number of hidden layers.\n",
        "        :param out_channels: The number of output channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert hidden_layers_num > 1\n",
        "        layers = [\n",
        "            nn.Linear(drug_channels * 2, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(num_features=hidden_channels, affine=True, momentum=None),\n",
        "            nn.ReLU(),\n",
        "        ]\n",
        "        for _ in range(hidden_layers_num - 1):\n",
        "            layers.extend(\n",
        "                [\n",
        "                    nn.Linear(hidden_channels, hidden_channels),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(num_features=hidden_channels, affine=True, momentum=None),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "            )\n",
        "        layers.extend([nn.Linear(hidden_channels, out_channels), nn.Sigmoid()])\n",
        "        self.final = nn.Sequential(*layers)\n",
        "\n",
        "    def unpack(self, batch: DrugPairBatch):\n",
        "        \"\"\"Return the context features, left drug features and right drug features.\"\"\"\n",
        "        return (\n",
        "            batch.drug_features_left,\n",
        "            batch.drug_features_right,\n",
        "        )\n",
        "\n",
        "    def _combine_sides(self, left: torch.FloatTensor, right: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        return torch.cat([left, right], dim=1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        drug_features_left: torch.FloatTensor,\n",
        "        drug_features_right: torch.FloatTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Run a forward pass of the DeepDDI model.\n",
        "\n",
        "        :param drug_features_left: A matrix of head drug features.\n",
        "        :param drug_features_right: A matrix of tail drug features.\n",
        "        :returns: A column vector of predicted interaction scores.\n",
        "        \"\"\"\n",
        "        hidden = self._combine_sides(drug_features_left, drug_features_right)\n",
        "        return self.final(hidden)\n",
        "# Hyperparameter 1: Hidden_Channels. Values = [128, 256, 1024, 2048 (used by DeepDDI by default)]\n",
        "model_2048 = DeepDDI(drug_channels=256).to(device)\n",
        "model_1024 = DeepDDI(drug_channels=256, hidden_channels=1024).to(device)\n",
        "model_128 = DeepDDI(drug_channels=256, hidden_channels=128).to(device)\n",
        "model_256 = DeepDDI(drug_channels=256, hidden_channels=256).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P_vnTgzk6v9"
      },
      "source": [
        "##   Evaluation Implementation\n",
        "- Definition and implementation of evaluation methods\n",
        "\n",
        "- Metrics descriptions:\n",
        "\n",
        "    - `Accuracy` : measures the proportion of correctly predicted instances out of the total instances. It is calculated by dividing the number of correct predictions by the total number of predictions.\n",
        "    - `Precision`: Also known as positive predictive value, precision is the number of true positive predictions divided by the number of all positive predictions (true positives plus false positives). It measures the accuracy of positive predictions.\n",
        "    - `Recall`: Also called sensitivity, recall is the number of true positive predictions divided by the number of actual positive instances (true positives plus false negatives). It measures the ability of the classifier to find all positive instances.\n",
        "    - `F1-Score`: F1-score is the harmonic mean of precision and recall. It provides a single score that balances both precision and recall. F1-score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
        "    - `Root Mean Squared Error`: a commonly used metric for evaluating the accuracy of a regression model. It measures the average of the squares of the errors, where the error is the difference between the predicted value and the actual value. Then, it takes the square root of this average. Essentially, it gives an idea of how spread out the errors are.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bTMDeLYEkwrC"
      },
      "outputs": [],
      "source": [
        "def classification_metrics(Y_pred, Y_true):\n",
        "    acc, precision, recall, f1score, rmse = accuracy_score(Y_true, Y_pred), \\\n",
        "                                          precision_score(Y_true, Y_pred), \\\n",
        "                                          recall_score(Y_true, Y_pred), \\\n",
        "                                          f1_score(Y_true, Y_pred), \\\n",
        "                                          root_mean_squared_error(Y_true, Y_pred)\n",
        "    return acc,  precision, recall, f1score, rmse\n",
        "\n",
        "\n",
        "\n",
        "#input: model, threshold\n",
        "def evaluate(model, threshold = 0.47):\n",
        "    model.eval()\n",
        "\n",
        "    generator.labeled_triples = test\n",
        "\n",
        "    predictions = []\n",
        "    for batch in generator:\n",
        "      batch = batch.to(device)\n",
        "      prediction = model(batch.drug_features_left, batch.drug_features_right)\n",
        "      prediction = prediction.detach().cpu().numpy()\n",
        "      identifiers = batch.identifiers\n",
        "      identifiers[\"prediction\"] = prediction\n",
        "      predictions.append(identifiers)\n",
        "      \n",
        "      \n",
        "    predictions_df = pd.concat(predictions)\n",
        "    Y_true = predictions_df[\"label\"]\n",
        "    Y_pred = (predictions_df[\"prediction\"] > threshold).astype(int)\n",
        "    acc, precision, recall, f1, rmse = classification_metrics(Y_true, Y_pred)\n",
        "    return acc, precision, recall, f1, rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSaWteNAdwtZ"
      },
      "source": [
        "##   Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-uKhb2DaedDq"
      },
      "outputs": [],
      "source": [
        "def train_model_(num_epochs=100, model=None,optimizer=None,\\\n",
        "                loss = torch.nn.BCELoss()):\n",
        "  # Hyperparameter 3: Classification threshold. Values = [0.44, 0.47 (default), 0.50]. Want to analyze performance with half the epochs \n",
        "  threshold_values = [0.44, 0.47, 0.50]\n",
        "  if optimizer == None:\n",
        "    optimizer= torch.optim.Adam(model.parameters())\n",
        "    \n",
        "  train_loss_arr = []\n",
        "  metrics_results = []\n",
        "  for _ in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss_value = 0\n",
        "    for batch in generator:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Move data to GPU\n",
        "      drug_features_left = batch.drug_features_left.to(device)\n",
        "      drug_features_right = batch.drug_features_right.to(device)\n",
        "      labels = batch.labels.to(device)\n",
        "      \n",
        "      prediction = model(drug_features_left, drug_features_right)\n",
        "\n",
        "      loss_value = loss(prediction, labels)\n",
        "      loss_value.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      train_loss_value += loss_value.item()\n",
        "    train_loss = train_loss_value / len(generator)\n",
        "\n",
        "    train_loss_arr.append(train_loss)\n",
        "    for threshold_value in threshold_values:\n",
        "      metrics_results.append({threshold_value : evaluate(model, threshold_value)})\n",
        "\n",
        "  return train_loss_arr, metrics_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def results(trained_model, number_epochs, hidden_channels):\n",
        "  def plot_metric(ax, metric_list, label, title):\n",
        "    ax.plot(metric_list, label=label)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "  \n",
        "  metrics_results = trained_model[1]\n",
        "  train_loss_arr = trained_model[0]\n",
        "  for metric_result_per_threshold_item in metrics_results.items():\n",
        "    acc_list, precision_list, recall_list, f1_list, rmse_list = zip(*metric_result_per_threshold_item.value())\n",
        "\n",
        "    # metrics to evaluate my model\n",
        "    print(f\"Threshold: {metric_result_per_threshold_item.key()} Number of Epochs: {number_epochs} Hidden Channels: {hidden_channels} \\n ACC: {np.mean(np.array(list(acc_list))):.3f}, Precision: {np.mean(np.array(list(precision_list))):.3f},Recall: {np.mean(np.array(list(recall_list))):.3f}, F1: {np.mean(np.array(list(f1_list))):.3f}, RMSE: {np.mean(np.array(list(rmse_list))):.3f}\")\n",
        "\n",
        "    # plot figures to better show the results\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "    \n",
        "    plot_metric(axes[0, 0], train_loss_arr, \"Training Loss\", \"Model Loss During Training\")\n",
        "    plot_metric(axes[0, 1], list(acc_list), \"Accuracy\", \"Model Accuracy During Training\")\n",
        "    plot_metric(axes[0, 2], list(precision_list), \"Precision\", \"Precision During Training\")\n",
        "    plot_metric(axes[1, 0], list(recall_list), \"Recall\", \"Recall Value During Training\")\n",
        "    plot_metric(axes[1, 1], list(f1_list), \"F1 Score\", \"F1 Score During Training\")\n",
        "    plot_metric(axes[1, 2], list(rmse_list), \"RMSE Score\", \"RMSE Value During Training\")\n",
        "    # it is better to save the numbers and figures for your presentation.\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter 2: Number of Epochs. Values = [50, 100, 125]. Want to analyze performance with half the epochs \n",
        "epoch_values = [50, 100, 125]\n",
        "for epoch in epoch_values:\n",
        "    trained_model_128 = train_model_(num_epochs=epoch, model=model_128)\n",
        "    results(trained_model_128, epoch, 128)\n",
        "    trained_model_256 = train_model_(num_epochs=epoch, model=model_256)\n",
        "    results(trained_model_128, epoch, 256)\n",
        "    trained_model_1024= train_model_(num_epochs=epoch, model=model_1024)\n",
        "    results(trained_model_128, epoch, 1024)\n",
        "    trained_model_2048 = train_model_(num_epochs=epoch, model=model_2048)\n",
        "    results(trained_model_128, epoch, 2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the original paper, DeepDDI achieves a 92.4% accuracy when trained for 100 epochs and the hidden layers have 2048 nodes each. We were able to verify these results achieving 92.2% accuracy for our model with 2048 nodes. Moreover, we saw a steady increase in accuracy from 128 nodes initially to 2048 in the last model. Similarly, RMSE and Training loss decreased as the number of nodes in the layers increased while Precision, Recall and F1-Score increased. From this results we can conclude, that the DeepDDI model achieves best results with 2048 nodes in each one of the hidden layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "We were able to reproduce this paper following the model implementation and procedures from ChemicalX. Loading the data was very simple as was training the model given the template supplied by the library. In addition, leveraging the evaluation metrics from the `skelearn` library, we had a simple way of assessing the quality of our models. The most difficult part in terms of reproducibility was the set up. The models had to be trained offline because of the memory and CPU caps in Colab. Moreover, we had to use the `cuda` application to leverage an Nvidia GTXForce 4060 to aid during training. Without the GPU it took nine hours to train all four models, while with the GPU we were able to train the model in one hour. Also, the library does not seem to be maintained (it requires Python version 3.8). Some versions and pinned to older Python versions which were not compatible with the Colab Environment. We had to fork the repo (https://github.com/lucag2/chemicalx) and unpin the versions manually. During the next phase, we plan to run the models with different number of epochs and all combinations of nodes. In addition, we want to test the performance with values different than `0.47` for the threshold. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Ryu, J.Y.; Kim, H.U.; Lee, S.Y. Deep learning improves prediction of drug–drug and drug–food interactions. Proc. Natl. Acad. Sci. USA 2018, 115, E4304–E4311 https://doi.org/10.1073/pnas.1803294115\n",
        "\n",
        "2. Pirmohamed, M., & Orme, M. (1998). Drug interactions of clinical importance. Davies’s textbook of adverse drug reactions, 888-912.\n",
        "\n",
        "3. DM Qato, J Wilder, LP Schumm, V Gillet, GC Alexander, Changes in prescription and over-the-counter medication and dietary supplement use among older adults in the United States, 2005 vs 2011. JAMA Intern Med 176, 473–482 (2016) https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2500064\n",
        "\n",
        "4. F Cheng, Z Zhao, Machine learning-based prediction of drug-drug interactions by integrating drug phenotypic, therapeutic, chemical, and genomic properties. J Am Med Inform Assoc 21, e278–e286 (2014).\n",
        "\n",
        "5. A Gottlieb, GY Stein, Y Oron, E Ruppin, R Sharan, INDI: A computational framework for inferring drug interactions and their associated recommendations. Mol Syst Biol 8, 592 (2012).\n",
        "\n",
        "6. P Zhang, F Wang, J Hu, R Sorrentino, Label propagation prediction of drug-drug interactions based on clinical side effects. Sci Rep 5, 12339 (2015).\n",
        "\n",
        "7. S Vilar, et al., Similarity-based modeling in large-scale prediction of drug-drug interactions. Nat Protoc 9, 2147–2163 (2014).\n",
        "\n",
        "8. MA Yildirim, KI Goh, ME Cusick, AL Barabási, M Vidal, Drug-target network. Nat Biotechnol 25, 1119–1126 (2007).\n",
        "\n",
        "9. K Park, D Kim, S Ha, D Lee, Predicting pharmacodynamic drug-drug interactions through signaling propagation interference on protein-protein interaction networks. PLoS One 10, e0140816 (2015).\n",
        "\n",
        "10. J Huang, et al., Systematic prediction of pharmacodynamic drug-drug interactions through protein-protein-interaction network. PLoS Comput Biol 9, e1002998 (2013).\n",
        "\n",
        "11. Benedek Rozemberczki, Charles Tapley Hoyt, Anna Gogleva, Piotr Grabowski, Klas Karis, Andrej Lamov, Andriy Nikolov, Sebastian Nilsson, Michael Ughetto, Yu Wang, Tyler Derr, and Benjamin M. Gyori. 2022. ChemicalX: A Deep Learning Library for Drug Pair Scoring. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22). Association for Computing Machinery, New York, NY, USA, 3819–3828. https://doi.org/10.1145/3534678.3539023\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
